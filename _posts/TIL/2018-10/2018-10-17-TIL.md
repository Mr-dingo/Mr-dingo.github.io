---
layout: post
title: "TIL"
subtitle: "Machine Learning & Plan & STL"
tag: [TIL]
---

## **Machine Learning**
(Linear)hypothesis
리니어하게 가설을 세운다는 것은 리니어한 직선을 데이터 분포에 맞춘다 라는 것이다. 즉 어떤 직선이 가장 데이터의 분포를 잘 표현하는지를 찾는것이 학습을 하는것.<br/>
H(x)를 1차 방정식으로 생각하는것을 Linear hypothesis 라 한다.
<br/>cost function : 우리가 생각한 가설(hypo)와 실제 데이터의 값의 차이가 된다.
cost function은 실제로는 _cost(W,b)_ 가 된다. 즉, weight 와 bias에 관련된 함수라 할 수 있다. 우리의 Goal 은 cost 를 최소화 하는 것이다.

실제로 tensorflow 를 이용해서 linear regression 모델을 구현해보자.

```python
import tensorflow as tf

x_train = [1,2,3,4]
y_train = [1,2,3,4]

W = tf.Variable(tf.random_normal([1]), name = 'weight')
b = tf.Variable(tf.random_normal([1]), name = 'bias')

"""
tensorflow import 하고 사용할 트레이닝 데이터를 설정. 그리고 weight 를 선언. 학습시킬 weight,bias 는 `tf.variable`을 이용해서 하는데 이렇게 하기보다는 `placeholder`로 구현하게 되면 더욱 좋다. 그 이유는 `placeholder`를 이용하게 되면 미리 정해진 데이터셋을 run 할 수도 있고 내가 원하는 때에 데이터를 추가적으로 넣어서 run 할수 있기 때문이다.
"""

hypothesis = x_train * W + b
# 우리가 트레이닝 시킬 H(x)를 리니어 리그래션 모델로 설정.
# hypothesis 도 하나의 tensor 이다.

cost = tf.reduce_mean(tf.square(hypothesis - y_train))
# cost 도 하나의 텐서, 여기서는 Leat Square를 사용.
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

#실제로 우리가 실행해야할 것은 train tensor 이다. train tensor node 에 optimizer가 연결되어있고 optimizer 는 cost 에 연결되있으니까 결국 train을 run하게되면 cost 를 minimize 하고 optimizer 는 GD를 써서 수행하는 결과를 보인다.

sess = tf.Session()
sess.run(tf.global_variables_initializer())

#run 시킬 session 선언 : 세션은 실행환경을 캡슐화 한것이고 각종 자원 위에 그래프를 올려서 실행시키는 것. 그래프를 하나, 혹은 여러개의 프로세서에 올리고 연산을 실행할 수 있는 방법을 제공. run 연산의 결과를 numpy의 ndarray로 반환.

for step in range(2001):
    sess.run(train)
    if step % 20 == 0 :
        print(step,sess.run(cost) , sess.run(W),sess.run(b))
```
이게 오늘배운 전체 소스코드.
<br/>
그래서 위의 코드를 place holder 로 바꾼 버젼은 다음과 같다.
```python
import tensorflow as tf

W = tf.Variable(tf.random_normal([1]), name = 'weight')
b = tf.Variable(tf.random_normal([1]), name = 'bias')
x_train = tf.placeholder(tf.float32 , shape=[None])
y_train =  tf.placeholder(tf.float32 , shape=[None])

hypothesis = x_train * W + b

cost = tf.reduce_mean(tf.square(hypothesis - y_train))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001):
        cost_val , W_val , b_val, _ = sess.run([cost,W,b,train], feed_dict= { x_train : [1,2,3] , y_train :[1,2,3]})
        if step % 20 == 0 :
                print(step , cost_val,W_val,b_val)

print(sess.run(hypothesis , feed_dict={x_train : [5]}))
print(sess.run(hypothesis , feed_dict={x_train : [2.5]}))
print(sess.run(hypothesis , feed_dict={x_train : [1.5,3.5]}))
```
## **컴퓨터의 구조와 원리 2.0 11장 시스템버스**
공부 완료함. 시스템버스, 외부 버스에 대해서 배웠고 버스의 발전 역사에 대해서 배웠다. 그리고 현재 상용화 되어있는 PCIe , ISbus 에 대해서도 설명이 나와있었고 배우게 되었음. 사실 오늘은 후딱 읽고 다음장에 나오게되는 assembly 언어에 대해서 배우려고 함. 내일도 하나를 읽어야겠음.

## **C++ 11 STL**
오늘은 못했다. ㅠㅠ 내일은 꼭한다.

>세번째 TIL 은 여기까지... 내일은 꼭 STL 공부하고야만다... 그리고 서버쪽도 공부해야하는데 리눅스 바이블을 한번 쭉 돌리는것이 좋을듯..