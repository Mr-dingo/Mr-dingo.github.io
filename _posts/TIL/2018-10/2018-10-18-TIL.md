---
layout: post
title: "TIL"
tag: [TIL]
---

## **Machine Learning**
tensorflow 에서 optimizer 사용법을 배움. 관련 코드를 깃허브에 올렷음.
user defied cost function 에 대한 gradient 계산을 tensorflow 에서 자동으로 계산할 수 있으며 계산된 gradient 를 clamp하거나 하는 등의 편집도 허용되게 코드를 짤 수 있음.
- - -
Multi-variable 인 경우에 대한 내용을 Lec4 에서 배움.
matrix 를 이용하면 multi-variable 인 경우도 손쉽게 표현할 수 있다. 
$$
w_1 * x_1 + w_2 * x_2 + ... + w_n * x_n
$$
같은 수식을 H(X) = XW 처럼 matrix 로 표현 가능함.
<br/> 데이터 하나를 instance라고 부른다.instance가 여러개인 경우 matrix 에 instance를 모두 때려박은 뒤 계산이 가능함. instance의 variable 갯수만 알면 weight의 dim을 알 수 있음.
<br/><br/>

강의를 들으면서 신기한 tensorflow 기능에 대해서 알아냈다.<br/>
>(1 2 3) + (7 8 9)<br/>
>(4 5 6)<br/>

만약 이 위의 식처럼 matrix 인데 서로 다른 dimension 을 가진 경우 어떻게 될까.사실 일반적인 시각이라면 이건 안된다 라고 말할수있다. 그런데 tensorflow에서는 이게 가능하다. <br/>
이게 가능하게 되는 원리를 tensorflow document 에서 broadcasting 이라고 말했다. broadcasting document 번역한다면 다른 모양의 어레이에 대해서 수학적인 연살이 가능하도록 만드는 프로세스라고 한다.그래서 위의 식을 일반적으로는 계산이 불가능 하지만 tensorflow 안에서는 계산이 가능하도록 만든다. 그래서 tensorflow 안에서 위와 같은 식을 계산하도록 코드를 짜게되면 결과적으로는 다음과 같은 식을 수행하게 되는것이다.
>(1 2 3) + (7 8 9) = (8  10 12)<br/>
>(4 5 6)   (7 8 9)   (11 13 15)<br/>

lab4 의 코드를 보면서 이해가 안됫지만 broadcasting 이라는 기능에 대해 이해하고 난 뒤에는 이해되었다.

그리고 file load 에 대해서도 lab4-2 에 대해서 배움. batch 에 대해서는 아직 감이 안잡히므로 나중에 프로젝트를 할때 다시한번 보는게 좋을듯. 그리고 numpy 로 parsing 햇는데 pandas 도 한번 알아봐야할듯 하다.
다음 강의는 5강 부터...
## **컴퓨터의 구조와 원리 2.0 12장**


## **C++ 11 STL**
드디어 시작..
코드를 쳐보면서 정리했고 정리한 결과를 라인별로 포스팅하려했지만 너무 시간이 오래걸리는 관계로 주석으로 처리해서 공부하기로 했다. 다만 책을 보는것과는 달리 코드로 직접 써보는것이 공부가 훨씬 잘 되는 느낌이라 좋았다. 책의 모든 example 을 쳐볼수는 없었고 간단하게 쳐봤고 나중에 프로젝트에 적극 활용해야겠다. 다만 모든 코드를 치고 각각의 원리나 메모리 레벨까지 아는것은 시간이 조금 걸린다. 내가 유용하다고 판된되는 기능들에 대해서는 적극적으로 조사해보고 하루에 하나씩 포스팅해야겠다. 오늘 배운것중에서는 lamda 와 range_base_for 가 유용해 보였으며 이 중 lamda 에 대해서는 추후에 포스팅을 하려고한다.

## VIM keybinding
increase num = `Ctrl` + `a` <br/>
decrease num = `Ctrl` + `x`


>네번째 TIL 은 여기까지... 공부를 하다보니 ML 수업에 너무 시간을 많이 쏟는듯하다. 다음부터는 STL 공부후 하나의 개념을 포스팅하고 그 후에 ML에 대해 공부하고 남은 시간에 다른것을 해야겠다.