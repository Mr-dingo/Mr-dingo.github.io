---
layout: post
title: "TIL"
tag: [TIL]
---
# 딥러닝 강의
김성 교수님의 딥러닝 10강을 들었다.
새로 onenote 에 새로운 섹션(딥러닝강의용)을 만들어서 수업들은 것을 정리한 노트를 관리를 시작했다.
앞으로도 강의나 논문을 읽게 되었을때 정리용 노트를 onenote로 통합하여 관리하도록 하겠다.
<br>
오늘 배운 내용은 
* NN을 이용해 XOR 구현한 것에 대한 복습
* Deep & Wide NN을 구현하는 것에 대한 복습
* Vanishing gradient 에 대한 내용
    * 왜 vanishing gradient가 발생하는지
    * ReLU를 이용한 vanishing gradient 해결 방법
    * 기타 다른 activation function
* wieght 초기값의 중요성
    * RBM (restricted boltzman machine)
    * Xavier/He initialization
* Drop out
    * DeepNN에서 오버피팅이 발생했을때 해결하는 방법
        * 더 많은 training data
        * feature 갯수 줄이기
        * Regularization (wieght 값 줄이기)
        * Drop Out 방법
    * Drop out 은 러닝하는 도중에 일부러 몇몇 wight를 랜덤하게 끊어버리는것
    * Drop out 을 이용해서 overfitting 문제를 해결하기도 한다
    * 구현은 굉장히 간단하다.
* Ensemble(앙상블)
    * 학습시킬 수 있는 장비의 여건이 된다면 앙상블을 이용하는 것도 좋다.
    * 독립적으로 NN을 구성하고 각각을 따로 training시키고 나중에 하나로 합치는 것을 앙상블이라고 한다.
* 다양한 형태의 네트워크를 쌓자!(LEGO 블럭 처럼)
    * RNN,CNN, Fast forward 등 굉장히 다양한 모양의 NN이 존재한다.
    * 나만의 네트워크를 창의적으로 생각하고 구축해보자!!!




> onenote에 배운 내용을 간단하게 작성하면서 공부를 하니까 TIL을 작성할 때 어떤내용을 했었는지 복습하는데에 굉장히 도움이 된다. 앞으로도 꾸준히 note를 작성하는 습관을 길들여서 TIL을 작성할때 복습을 하는 식으로 진행하는게 좋겠다.